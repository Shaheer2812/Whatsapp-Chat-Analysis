{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WhatsApp Chat Analysis - NLP & Content Analysis\n",
                "\n",
                "This notebook focuses on the content of the messages: what words are used, which emojis are popular, and the overall sentiment of the conversation.\n",
                "\n",
                "## 1. Import Libraries & Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import plotly.express as px\n",
                "import sys\n",
                "import os\n",
                "import re\n",
                "from collections import Counter\n",
                "from wordcloud import WordCloud\n",
                "import matplotlib.pyplot as plt\n",
                "import emoji\n",
                "import nltk\n",
                "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
                "\n",
                "# Download VADER lexicon if not already present\n",
                "try:\n",
                "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
                "except LookupError:\n",
                "    nltk.download('vader_lexicon')\n",
                "\n",
                "# Add src to path to import parser\n",
                "sys.path.append(os.path.abspath('../src'))\n",
                "from parser import WhatsAppParser\n",
                "\n",
                "# Load Data\n",
                "file_path = '../data/WhatsApp Chat with gg bOys.txt'\n",
                "parser = WhatsAppParser(file_path)\n",
                "df = parser.parse()\n",
                "\n",
                "# Filter out \"Media omitted\" messages\n",
                "df_text = df[~df['Message'].str.contains('<Media omitted>', case=False, na=False)].copy()\n",
                "print(f\"Messages for Text Analysis: {len(df_text)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Word Frequency & Word Cloud\n",
                "What are the most common words? We need to remove stopwords to see meaningful content."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom Stopwords list (including Hinglish/Urdu terms)\n",
                "STOPWORDS = set(['the', 'is', 'in', 'to', 'and', 'a', 'of', 'for', 'it', 'I', 'you', 'my', 'that', 'on', 'with', 'this', 'be', 'at', \n",
                "                 'media', 'omitted', 'image', 'video', 'sticker', 'GIF', 'lol', 'ok', 'okay', 'yeah', 'yes', 'no', 'haha', 'message', 'deleted',\n",
                "                 'hai', 'ki', 'ke', 'ka', 'se', 'ko', 'aur', 'mai', 'to', 'bhi', 'tha', 'nahi', 'kya', 'kar', 'ho', 'ab', 'wo'])\n",
                "\n",
                "def clean_text(text):\n",
                "    # Simple cleaner: lowercase, remove non-alphabetic characters\n",
                "    text = str(text).lower()\n",
                "    text = re.sub(r'[^a-z\\s]', '', text)\n",
                "    return text\n",
                "\n",
                "all_text = ' '.join(df_text['Message'].apply(clean_text))\n",
                "words = [word for word in all_text.split() if word not in STOPWORDS and len(word) > 2]\n",
                "\n",
                "# Generate Word Cloud\n",
                "# Explicit font path to ensure compatibility across environments\n",
                "font_path = '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'\n",
                "try:\n",
                "    wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='viridis', font_path=font_path).generate(' '.join(words))\n",
                "except ValueError:\n",
                "    # Fallback if font not found\n",
                "    wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='viridis').generate(' '.join(words))\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.imshow(wordcloud, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('Most Common Words in Chat')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Emoji Analysis\n",
                "Who uses which emojis the most?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_emojis(text):\n",
                "    return [c for c in text if c in emoji.EMOJI_DATA]\n",
                "\n",
                "df['Emojis'] = df['Message'].apply(extract_emojis)\n",
                "all_emojis = [e for sublist in df['Emojis'] for e in sublist]\n",
                "\n",
                "emoji_counts = Counter(all_emojis).most_common(10)\n",
                "emoji_df = pd.DataFrame(emoji_counts, columns=['Emoji', 'Count'])\n",
                "\n",
                "fig_emoji = px.bar(emoji_df, x='Emoji', y='Count', title='Top 10 Most Used Emojis', \n",
                "                   template='plotly_dark', color='Count', color_continuous_scale='Magma')\n",
                "fig_emoji.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Sentiment Analysis (VADER)\n",
                "VADER (Valence Aware Dictionary and sEntiment Reasoner) is specifically tuned for social media sentiment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sia = SentimentIntensityAnalyzer()\n",
                "\n",
                "def get_sentiment(text):\n",
                "    # VADER works best on raw text (with emojis/caps), so we use the original message\n",
                "    return sia.polarity_scores(str(text))['compound']\n",
                "\n",
                "df_text['Sentiment'] = df_text['Message'].apply(get_sentiment)\n",
                "\n",
                "# Average Sentiment per User\n",
                "user_sentiment = df_text.groupby('Author')['Sentiment'].mean().reset_index().sort_values('Sentiment', ascending=False)\n",
                "\n",
                "fig_sent = px.bar(user_sentiment, x='Author', y='Sentiment', title='Average Sentiment Score per User (Positivity)',\n",
                "                  color='Sentiment', color_continuous_scale='RdBu', range_color=[-0.5, 0.5], template='plotly_dark')\n",
                "fig_sent.show()\n",
                "\n",
                "# Sentiment Over Time (Monthly Average)\n",
                "df_text['Month'] = df_text['DateTime'].dt.to_period('M').astype(str)\n",
                "monthly_sentiment = df_text.groupby('Month')['Sentiment'].mean().reset_index()\n",
                "\n",
                "fig_sent_time = px.line(monthly_sentiment, x='Month', y='Sentiment', title='Sentiment Trend Over Time',\n",
                "                        markers=True, template='plotly_dark')\n",
                "fig_sent_time.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}